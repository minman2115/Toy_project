{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "### [개요]\n",
    "\n",
    "치킨이미지를 인식하여 치킨브랜드(BBQ, 네네, 교촌 등)를 분류할 수 있는 CNN(MobileNet을 fine tuning한 형태) 모델 구현 및 AWS Endpoint 배포\n",
    "\n",
    "\n",
    "### [구현과정]\n",
    "\n",
    "STEP 1) 아이디어 도출\n",
    "\n",
    "STEP 2) 데이터수집 : 인스타그램에서 크롤러를 이용하여 치킨브랜드별로 키워드 검색하여 치킨이미지 수집\n",
    "\n",
    "STEP 3) 모델링 : AWS Sagemaker의 Tensorflow 2.0을 이용하여 치킨이미지를 분류할 수 있는 CNN(MobileNet을 fine tuning한 형태) 구현\n",
    "\n",
    "STEP 4) 트레이닝 및 배포\n",
    "\n",
    "\n",
    "### [구현결과]\n",
    "\n",
    "#### STEP 1) 아이디어 도출\n",
    "\n",
    "- 인스타그램에서 크롤링한 치킨이미지 데이터를 input 값으로 넣어주면 ouput 값으로 치킨브랜드 이름을 분류하는 치믈리에 머신러닝 구현\n",
    "\n",
    "#### STEP 2) 데이터 수집\n",
    "\n",
    "- 인스타그램 웹크롤러를 이용한 데이터 수집\n",
    "\n",
    "\n",
    "- 사용한 웹크롤러 : https://github.com/huaying/instagram-crawler\n",
    "\n",
    "\n",
    "- 이슈 : 순수한 치킨 사진 외 관련 없는 사진도 수집되는 현상 식별\n",
    "\n",
    "\n",
    "- 해결방안 : 순수한 치킨 사진 감별 및 불필요한 사진 데이터 제거\n",
    "\n",
    "\n",
    "결론적으로 4개의 브랜드(BBQ, 굽네, 교촌, 네네), 브랜드별 170장의 치킨이미지 데이터 수집\n",
    "\n",
    "#### STEP 3) 모델링\n",
    "\n",
    "- 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "import os, sys\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "from keras.utils import np_utils\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "## Target 데이터에 대한 라벨링을 수행하는 함수\n",
    "def Label_encoding(target_data):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(target_data)\n",
    "    arget_data = le.transform(target_data)\n",
    "    target_data = np_utils.to_categorical(target_data, len(brend_names))\n",
    "    return target_data\n",
    "\n",
    "## 데이터 전처리를 수행하는 함수\n",
    "def process_data():\n",
    "    \n",
    "    brend_names = ['BBQ','goobne', 'kyochon', 'nene']\n",
    "\n",
    "    img_data = []\n",
    "    target_data = []\n",
    "\n",
    "    # 이미지 데이터 탑재한 S3에 접근\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    \n",
    "    # 브랜드별 이미지 처리\n",
    "    for brend in brend_names:\n",
    "\n",
    "        s3_data_list = fs.ls('s3://lhw-s3-test/sagemaker_test/brend/' + brend)\n",
    "\n",
    "        for data in s3_data_list[0:170+1]:\n",
    "            with fs.open('s3://{}'.format(data)) as f:\n",
    "                img = Image.open(f)\n",
    "\n",
    "                resize_img = img.resize((240,240))\n",
    "                array_of_img = np.array(resize_img)\n",
    "\n",
    "                img_data.append(array_of_img.reshape(240,240, 3).astype('float32') / 255.0)\n",
    "                target_data.append(brend)\n",
    "                \n",
    "    img_data = np.array(img_data)\n",
    "    \n",
    "    target_data = Label_encoding(target_data)\n",
    "    \n",
    "    # train, test split\n",
    "    x, y = shuffle(img_data, target_data, random_state=42)\n",
    "\n",
    "    x_train = x[:600+1]\n",
    "    y_train = y[:600+1]\n",
    "\n",
    "    x_test = x[600:]\n",
    "    y_test = y[600:]\n",
    "    \n",
    "    np.save('train_data', x_train)\n",
    "    np.save('train_labels', y_train)\n",
    "    np.save('eval_data', x_test)\n",
    "    np.save('eval_labels', y_test)\n",
    "    \n",
    "    \n",
    "    ## s3에 전처리한 데이터를 저장\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Object('lhw-s3-test', 'sagemaker_test/dataset/train_data.npy').put(Body=open('train_data.npy', 'rb'))\n",
    "    s3.Object('lhw-s3-test', 'sagemaker_test/dataset/train_labels.npy').put(Body=open('train_labels.npy', 'rb'))\n",
    "    s3.Object('lhw-s3-test', 'sagemaker_test/dataset/eval_data.npy').put(Body=open('eval_data.npy', 'rb'))\n",
    "    s3.Object('lhw-s3-test', 'sagemaker_test/dataset/eval_labels.npy').put(Body=open('eval_labels.npy', 'rb'))\n",
    "                                         \n",
    "    return None\n",
    "\n",
    "process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델링(chick_learning.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "import os, sys\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from tensorflow.keras.layers import Conv2D, ReLU, MaxPooling2D, Dense, BatchNormalization, Softmax, GlobalAveragePooling2D\n",
    "\n",
    "def model(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    learning_rate = 0.0002\n",
    "    batch_size = 16\n",
    "    n_train = 601\n",
    "    n_class = 4\n",
    "    \n",
    "    conv_base = MobileNet(weights='imagenet',include_top=False,input_shape=(240, 240, 3))\n",
    "    model = models.Sequential()\n",
    "    model.add(conv_base)\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    model.add(Dense(n_class))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Softmax())\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0002, \n",
    "                                                              decay_steps=n_train//batch_size*5,\n",
    "                                                              decay_rate=0.5,\n",
    "                                                              staircase=True)\n",
    "    model.compile(optimizers.Adam(lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs = 10, batch_size = 10, validation_data = (x_test,y_test))\n",
    "    model.evaluate(x_test, y_test)\n",
    "    return model\n",
    "\n",
    "def _load_training_data(base_dir):\n",
    "    x_train = np.load(os.path.join(base_dir, 'train_data.npy'))\n",
    "    y_train = np.load(os.path.join(base_dir, 'train_labels.npy'))\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "def _load_testing_data(base_dir):\n",
    "    x_test = np.load(os.path.join(base_dir, 'eval_data.npy'))\n",
    "    y_test = np.load(os.path.join(base_dir, 'eval_labels.npy'))\n",
    "    return x_test, y_test\n",
    "\n",
    "\n",
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    # model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\n",
    "    parser.add_argument('--model_dir', type=str)\n",
    "    parser.add_argument('--sm-model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))\n",
    "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ.get('SM_HOSTS')))\n",
    "    parser.add_argument('--current-host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n",
    "\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args, unknown = _parse_args()\n",
    "    \n",
    "    train_data, train_labels = _load_training_data(args.train)\n",
    "    eval_data, eval_labels = _load_testing_data(args.train)\n",
    "    \n",
    "    CNN_classifier = model(train_data, train_labels, eval_data, eval_labels)\n",
    "\n",
    "    if args.current_host == args.hosts[0]:\n",
    "        # save model to an S3 directory with version number '00000001'\n",
    "        CNN_classifier.save(os.path.join(args.sm_model_dir, '000000001'), 'my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 4) 트레이닝 및 배포\n",
    "\n",
    "- 트레이닝 (Sagemaker 훈련 및 모델등록)\n",
    "\n",
    "아래 코드를 실행하면 모델이 트레이닝되고 등록이 된다. 그리고 S3에 훈련이 완료된 모델이 저장된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "training_data_uri = 's3://lhw-s3-test/sagemaker_test/dataset'\n",
    "\n",
    "CNN_estimator = TensorFlow(entry_point='chick_learning.py',\n",
    "                             role=role,\n",
    "                             train_instance_count=1,\n",
    "                             train_instance_type='ml.p2.xlarge',\n",
    "                             framework_version='2.0.0',\n",
    "                             py_version='py3',\n",
    "                             distributions={'parameter_server': {'enabled': True}})\n",
    "\n",
    "CNN_estimator.fit(training_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](https://user-images.githubusercontent.com/41605276/75853841-3a6fc280-5e32-11ea-9fa2-06fd819c68dd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배포(Sagemaker 앤드포인트 등록)\n",
    "\n",
    "아래 코드를 실행하면 S3에 훈련이 완료된 모델을 베이스로 Sagemaker 앤드포인트를 생성하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = CNN_estimator.deploy(initial_instance_count=1, instance_type='ml.p2.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2](https://user-images.githubusercontent.com/41605276/75853869-42c7fd80-5e32-11ea-9cef-701e357161a8.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
